"""Mission 3 - Chatbot IA local (LLM compact via llama.cpp)."""
import streamlit as st
from openai import OpenAI
from utils import badges


# -----------------------------------------------------------------------------
# Configuration LLM (llama.cpp server en mode OpenAI-compatible)
# -----------------------------------------------------------------------------
BASE_URL = "http://localhost:8033/v1"
LLM_MODEL = "mistral"

SYSTEM_PROMPT = (
    "Tu es un assistant IA local. "
    "R√©ponds en fran√ßais, de mani√®re claire et structur√©e. "
    "Si l'utilisateur demande du code, donne un exemple minimal et correct."
)


@st.cache_resource
def get_llm_client() -> OpenAI:
    # llama.cpp (server) accepte un api_key factice en mode local
    return OpenAI(base_url=BASE_URL, api_key="sk-no-key-needed")


def _init_state():
    if "m3_messages" not in st.session_state:
        st.session_state.m3_messages = []
    if "m3_system_prompt" not in st.session_state:
        st.session_state.m3_system_prompt = SYSTEM_PROMPT
    if "m3_badge_unlocked" not in st.session_state:
        st.session_state.m3_badge_unlocked = False


def _render_history():
    for msg in st.session_state.m3_messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])


def _unlock_badge_once():
    if not st.session_state.m3_badge_unlocked:
        badges.unlock_badge("mission3")
        st.session_state.m3_badge_unlocked = True
        st.success("üéâ F√©licitations, vous avez d√©bloqu√© le badge **Chatbot** üí¨")


def run():
    _init_state()

    st.title("Mission 3 : Chatbot IA local üí¨")
    st.write(
        "**Objectif** : Interagir avec un **assistant IA** fonctionnant enti√®rement en local "
        "(via un serveur llama.cpp compatible OpenAI)."
    )

    # Zone de configuration (sans sidebar)
    with st.expander("‚öôÔ∏è Prompt syst√®me (optionnel)", expanded=False):
        st.text_area(
            "Le prompt syst√®me guide le comportement de l'assistant.",
            key="m3_system_prompt",
            height=160,
        )
        c1, c2 = st.columns(2)
        with c1:
            if st.button("R√©initialiser le prompt syst√®me"):
                st.session_state.m3_system_prompt = SYSTEM_PROMPT
                st.rerun()
        with c2:
            if st.button("üßπ Effacer la conversation"):
                st.session_state.m3_messages = []
                st.rerun()

    # Affichage historique
    _render_history()

    # Le chat_input DOIT √™tre le dernier √©l√©ment
    prompt = st.chat_input("Posez votre question...")
    if prompt:
        st.session_state.m3_messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        client = get_llm_client()

        # Construction du contexte: system + historique
        messages_for_llm = [{"role": "system", "content": st.session_state.m3_system_prompt}]
        messages_for_llm.extend(st.session_state.m3_messages)

        # Appel LLM en streaming (comme app_llamacpp_v3.py)
        with st.chat_message("assistant"):
            try:
                stream = client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=messages_for_llm,
                    stream=True,
                    temperature=0.3,
                    top_p=0.9,
                    presence_penalty=0.6,
                    frequency_penalty=1.5,
                    max_tokens=2048,
                )
                response = st.write_stream(stream)
            except Exception as exc:
                response = (
                    "‚ùå Impossible de contacter le serveur llama.cpp.\n\n"
                    f"**D√©tail** : {exc}\n\n"
                    "V√©rifiez que le serveur est lanc√© et accessible sur : "
                    f"`{BASE_URL}` (endpoint `/chat/completions`)."
                )
                st.error(response)

        st.session_state.m3_messages.append({"role": "assistant", "content": response})

        # Badge: consid√©r√© ‚Äúaccompli‚Äù apr√®s au moins une interaction
        _unlock_badge_once()

    # Navigation
    st.divider()
    if st.button("Accueil ‚û°Ô∏è"):
        st.session_state.page = "introduction"
